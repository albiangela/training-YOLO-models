{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e45ed01-7c19-4318-a669-a0334df95115",
   "metadata": {},
   "source": [
    "# Train Custom YOLO Models for Object Detection\n",
    "\n",
    "This notebook is designed to help you train your own YOLO object detection models from scratch or fine-tune existing ones.  \n",
    "You can use either:\n",
    "\n",
    "- **Local datasets** (e.g., in YOLO format stored on your Google Drive or GitHub)\n",
    "- **Datasets from Roboflow**, which can be easily imported via a download link\n",
    "\n",
    "The workflow includes:\n",
    "- Loading and organizing your dataset\n",
    "- Writing a custom `.yaml` config file\n",
    "- Launching training with the `ultralytics` YOLO implementation\n",
    "- (Optional) Exporting and evaluating your trained model\n",
    "\n",
    "This is ideal for training models on custom objects — whether you're working with animals, vehicles, tools, or underwater footage.\n",
    "\n",
    "---\n",
    "\n",
    "Make sure your dataset is in the correct YOLO structure:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── train/\n",
    "│   ├── images/\n",
    "│   └── labels/\n",
    "├── valid/\n",
    "│   ├── images/\n",
    "│   └── labels/\n",
    "├── test/   # optional\n",
    "│   ├── images/\n",
    "│   └── labels/\n",
    "└── data.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892d97b-d9d7-493e-8285-5770875b300d",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d51667-95ca-4a74-badf-f3cfa9c72e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 🚀 Python-3.12.7 torch-2.5.1.post4 CPU (Apple M2 Pro)\n",
      "Setup complete ✅ (12 CPUs, 32.0 GB RAM, 880.9/926.4 GB disk)\n",
      "/Users/ang/Seafile/TRex-tutorials-data/code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import math\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "import numpy as np\n",
    "import time  # Import the time module\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "from utils import *\n",
    "\n",
    "# from google.colab import runtime\n",
    "# from google.colab import drive\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e448cc-e986-4909-bda8-51225977917a",
   "metadata": {},
   "source": [
    "#### There are options: 1) to load data from local folder (.zip folder) 2) to get data directly from roboflow\n",
    "Chose the one that applies to your case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fcf1e-a0a9-4257-ab7f-c38b74d1c376",
   "metadata": {},
   "source": [
    "# 1) Load data from local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2174f5-a2c8-4f16-ac19-e31f9e4ebee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /content: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'TRex-tutorials-data'...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# USER OPTION: Choose your data source\n",
    "# -------------------------------\n",
    "USE_LOCAL = False  # Set to True if loading from Google Drive\n",
    "\n",
    "# -------------------------------\n",
    "# Setup\n",
    "# -------------------------------\n",
    "import os\n",
    "\n",
    "# Clean and prepare dataset directory\n",
    "!rm -rf /content/datasets\n",
    "!mkdir /content/datasets\n",
    "\n",
    "if USE_LOCAL:\n",
    "    # --- Option 1: Load from Google Drive ---\n",
    "    dir_name = \"/Users/ang/Google Drive//models/hexbugs/\"\n",
    "    name = \"hexseg.v2i.yolov11\"\n",
    "\n",
    "    # Unzip the dataset\n",
    "    !unzip {dir_name}{name}.zip -d /content/datasets\n",
    "\n",
    "else:\n",
    "\n",
    "    # --- Option 2: Clone from GitHub (for local Jupyter setup) ---\n",
    "    name = \"hexbugs-annotation-dataset\"\n",
    "    repo_url = \"https://github.com/albiangela/TRex-tutorials-data.git\"\n",
    "    local_base = Path.cwd() / \"datasets\"\n",
    "    dataset_path = local_base / name\n",
    "    \n",
    "    # Ensure the datasets folder exists\n",
    "    local_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Clone the repository\n",
    "    os.system(f\"git clone {repo_url}\")\n",
    "    \n",
    "    # Copy the dataset folder to the local datasets directory\n",
    "    source_folder = Path(\"TRex-tutorials-data/YOLO-models\") / name\n",
    "    shutil.copytree(source_folder, dataset_path, dirs_exist_ok=True)\n",
    "    \n",
    "    # Unzip dataset (assumes only one ZIP file in the folder)\n",
    "    zip_files = list(dataset_path.glob(\"*.zip\"))\n",
    "    if zip_files:\n",
    "        with zipfile.ZipFile(zip_files[0], 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_path)\n",
    "        zip_files[0].unlink()  # remove zip after extraction\n",
    "    \n",
    "    # Move unzipped contents up one level if nested in a subfolder\n",
    "    nested_folder = dataset_path / name\n",
    "    if nested_folder.exists() and nested_folder.is_dir():\n",
    "        for item in nested_folder.iterdir():\n",
    "            shutil.move(str(item), str(dataset_path))\n",
    "        shutil.rmtree(nested_folder)\n",
    "    \n",
    "    # Clean up cloned repo\n",
    "    shutil.rmtree(\"TRex-tutorials-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ee34d2-46fd-4085-8058-95e91238997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset location: /Users/ang/Seafile/TRex-tutorials-data/code/datasets/hexbugs-annotation-dataset\n"
     ]
    }
   ],
   "source": [
    "# Define a simple class to hold dataset metadata\n",
    "class Data:\n",
    "    def __init__(self, location, name, version=1):\n",
    "        self.location = Path(location).resolve()  # Convert to absolute Path object\n",
    "        self.version = version                    # Optional versioning\n",
    "        self.name = name                          # Name of the dataset\n",
    "\n",
    "# Create an instance of the Data class with the path and name of the dataset\n",
    "dataset = Data(Path(\"datasets\") / name, name)\n",
    "\n",
    "# Assert that the 'train' folder exists within the dataset location\n",
    "# This will raise an error if the folder is missing\n",
    "assert (dataset.location / \"train\").exists(), f\"'train' folder not found in {dataset.location}\"\n",
    "\n",
    "# Return or print the dataset location\n",
    "print(\"Dataset location:\", dataset.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98405ee-3db5-46ce-b23e-d35c132098ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "prepare_yolo_dataset(\n",
    "    dataset_path=dataset.location,  # your original dataset object\n",
    "    output_path=str(output_path),   # convert Path object to string\n",
    "    split=(0.7, 0.2, 0.1),          # train, valid, test\n",
    "    remove_test=False               # keep test if it exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba9e383-e5c6-44fb-a221-724daaf21022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebalanced dataset saved to: /Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset\n",
      "Train: 28 | valid: 8 | test: 5\n",
      "Dataset prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set output path to a new folder named 'rebalanced_dataset' in the current working directory\n",
    "output_path = Path.cwd() / \"rebalanced_dataset\"\n",
    "\n",
    "prepare_yolo_dataset(\n",
    "    dataset_path=dataset.location,  # your original dataset object\n",
    "    output_path=str(output_path),   # convert Path object to string\n",
    "    split=(0.7, 0.2, 0.1),          # train, valid, test\n",
    "    remove_test=False               # keep test if it exists\n",
    ")\n",
    "\n",
    "# ## If you want to change the labels, the function will look more like this\n",
    "# prepare_yolo_dataset(\n",
    "#     dataset_path=dataset.location,  # your original dataset object\n",
    "#     output_path=str(output_path),   # convert Path object to string\n",
    "#     split=(0.7, 0.2, 0.1),\n",
    "#     remove_test=False,\n",
    "#     allowed_ids={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n",
    "#     collapse_map=collapse_map,\n",
    "#     new_class_ids=new_class_ids,\n",
    "#     drop_others=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd59677a-3800-4c46-884a-60e7d0ebb46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: Counter({0: 140})\n",
      "Valid class counts: Counter({0: 40})\n",
      "Test class counts: Counter({0: 25})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_labels(label_dir):\n",
    "    class_counts = Counter()\n",
    "    for fname in os.listdir(label_dir):\n",
    "        if not fname.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(label_dir, fname)) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts:\n",
    "                    try:\n",
    "                        class_id = int(parts[0])\n",
    "                        class_counts[class_id] += 1\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    return class_counts\n",
    "\n",
    "# Check balance\n",
    "print(\"Train class counts:\", count_labels(Path.cwd() / 'rebalanced_dataset/train/labels'))\n",
    "print(\"Valid class counts:\", count_labels(Path.cwd() / 'rebalanced_dataset/valid/labels'))\n",
    "print(\"Test class counts:\", count_labels(Path.cwd() / 'rebalanced_dataset/test/labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d988123-ef30-47bb-aa7c-566b2285e683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a simple class to hold dataset metadata\n",
    "class Data:\n",
    "    def __init__(self, location, name, version=1):\n",
    "        self.location = Path(location).resolve()  # Ensure it's a full path\n",
    "        self.version = version\n",
    "        self.name = name\n",
    "\n",
    "# Try to use a named rebalanced dataset with previous name as prefix, fallback if dataset.name is not defined\n",
    "try:\n",
    "    dataset = Data(Path.cwd() / \"rebalanced_dataset\", dataset.name + \"_rebalanced\")\n",
    "except NameError:\n",
    "    print(\"⚠️ 'dataset.name' not defined. Falling back to default name 'rebalanced'.\")\n",
    "    dataset = Data(Path.cwd() / \"rebalanced_dataset\", \"rebalanced\")\n",
    "\n",
    "# Assert that the 'train' folder exists\n",
    "assert (dataset.location / \"train\").exists(), f\"'train' folder not found in: {dataset.location}\"\n",
    "\n",
    "# Show the resolved dataset path\n",
    "dataset.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b86079-2688-41ac-8afe-4474ab4a302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Copied dataset config from /Users/ang/Seafile/TRex-tutorials-data/code/datasets/hexbugs-annotation-dataset/data.yaml to /Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset/data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Define source and destination paths\n",
    "source_yaml = Path(dataset_path) / \"data.yaml\"  # or \"dataset.yaml\" if that's the actual name\n",
    "if not source_yaml.exists():\n",
    "    source_yaml = Path(dataset_path) / \"dataset.yaml\"  # fallback if using a different name\n",
    "\n",
    "destination_yaml = dataset.location / \"data.yaml\"\n",
    "\n",
    "# Copy the YAML file\n",
    "shutil.copy(source_yaml, destination_yaml)\n",
    "\n",
    "print(f\"✅ Copied dataset config from {source_yaml} to {destination_yaml}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a5a15-562c-43cd-bd3e-f84091052ef2",
   "metadata": {},
   "source": [
    "# 4) Define Training paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3bf4cd3-12af-4000-975c-af88e41c8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in directory: /Users/ang/Seafile/TRex-tutorials-data/code\n",
      "✅ Directory '/Users/ang/Seafile/TRex-tutorials-data/code/models/rocks' created.\n"
     ]
    }
   ],
   "source": [
    "# Define your local output folder for saving models\n",
    "REMOTE_URL = Path.cwd() / \"models\" / \"rocks\"\n",
    "HOME = Path.cwd()\n",
    "\n",
    "# Change to HOME directory (optional in local Jupyter, just informative here)\n",
    "print(f\"Working in directory: {HOME}\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not REMOTE_URL.exists():\n",
    "    REMOTE_URL.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✅ Directory '{REMOTE_URL}' created.\")\n",
    "else:\n",
    "    print(f\"📁 Directory '{REMOTE_URL}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ca541c8-4d42-43d6-b961-891a89f2db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ang/Seafile/TRex-tutorials-data/code\n",
      "🔧 Training params: translate=0.25 mixup=0.001 copy_paste=0.15 scale=0.25 mosaic=1 close_mosaic=10 line_width=1 optimize=True dynamic=True format=tflite nms=True half=False plots=True cache=disk single_cls=False amp=True augment=False workers=16 degrees=0 flipud=0.0 fliplr=0.0\n",
      "🧪 resolution=1980 | project=1980-yolo11n-seg-mosaic | date_string=2025-06-10-00_hexbugs-annotation-dataset_rebalanced_rebalanced-1\n",
      "📦 model=yolo11n-seg | base_model=yolo11n-seg | task=segment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change to home directory\n",
    "%cd {HOME}\n",
    "\n",
    "# ---- User-defined Settings ----\n",
    "resolution = 1980                # Image resolution for training\n",
    "epochs = 100                     # Number of training epochs\n",
    "batch_size = 4                   # Batch size\n",
    "base_model = \"yolo11n-seg\"       # Choose model variant. Options: \"yolo11n-pose\", \"yolo11n-seg\", \"yolo11n\" etc.\n",
    "cropped = False                  # Whether images are cropped (affects naming only)\n",
    "# sharkcam = True                  # Used for naming (optional toggle)\n",
    "\n",
    "\n",
    "# ---- Auto-detect task type ----\n",
    "if \"-seg\" in base_model:\n",
    "    task = \"segment\"\n",
    "elif \"-pose\" in base_model:\n",
    "    task = \"pose\"\n",
    "else:\n",
    "    task = \"detect\"\n",
    "\n",
    "# ---- 🔧 Training Settings ----\n",
    "common_settings = {\n",
    "    \"translate\": 0.25,       # Maximum image translation as data augmentation (in % of image size)\n",
    "    \"mixup\": 0.001,          # MixUp blending factor for image mixing (usually low for object detection)\n",
    "    \"copy_paste\": 0.15,      # Probability of using Copy-Paste augmentation (object pasting)\n",
    "    \"scale\": 0.25,           # Random scaling of images for augmentation\n",
    "    \"mosaic\": 1,             # Enable Mosaic augmentation (combines 4 images into 1)\n",
    "    \"close_mosaic\": 10,      # Number of epochs before disabling mosaic for better fine-tuning\n",
    "    \"line_width\": 1,         # Line width for label visualization\n",
    "    \"optimize\": True,        # Apply training graph optimization\n",
    "    \"dynamic\": True,         # Dynamic input resizing (True enables better memory usage)\n",
    "    \"format\": \"tflite\",      # Export model format (e.g., 'tflite', 'onnx', 'torchscript')\n",
    "    \"nms\": True,             # Apply Non-Maximum Suppression during inference\n",
    "    \"half\": False,           # Use 16-bit (half) precision if supported\n",
    "    \"plots\": True,           # Save training plots (loss, mAP, etc.)\n",
    "    \"cache\": \"disk\",         # Caching mode: \"disk\" to speed up I/O\n",
    "    \"single_cls\": False,     # If True, treat all objects as one class (for class-agnostic detection)\n",
    "    \"amp\": True,             # Enable automatic mixed precision (reduces memory, speeds up training)\n",
    "    \"augment\": False,        # If True, applies augmentation at inference time\n",
    "    \"workers\": 16            # Number of dataloader workers (adjust depending on your CPU)\n",
    "}\n",
    "\n",
    "# Modify task-specific augmentations\n",
    "if task == \"detect\":\n",
    "    common_settings.update({\n",
    "        \"degrees\": 180,       # Allow full rotation\n",
    "        \"flipud\": 0.25,       # Vertical flip probability\n",
    "        \"fliplr\": 0.25        # Horizontal flip probability\n",
    "    })\n",
    "else:\n",
    "    common_settings.update({\n",
    "        \"degrees\": 0,         # No rotation for pose/seg\n",
    "        \"flipud\": 0.0,\n",
    "        \"fliplr\": 0.0\n",
    "    })\n",
    "\n",
    "# Print CLI training parameters\n",
    "parms = \" \".join([f\"{k}={v}\" for k, v in common_settings.items()])\n",
    "print(\"🔧 Training params:\", parms)\n",
    "\n",
    "# ---- 🗂️ Model Output Naming ----\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_string = now.strftime(\"%Y-%m-%d-%H\") + \"_\" + dataset.name.replace(\" \", \"-\") + \"-\" + str(dataset.version)\n",
    "\n",
    "project = f\"{resolution}-{base_model}\"\n",
    "if common_settings[\"mosaic\"] > 0:\n",
    "    project += \"-mosaic\"\n",
    "if cropped:\n",
    "    project += \"-cropped\"\n",
    "# if sharkcam:\n",
    "#     project += \"-sharkcam\"  # Add logic if needed\n",
    "\n",
    "# ---- 🧠 Model Weights Source ----\n",
    "model = base_model  # or path to a pretrained model\n",
    "print(f\"🧪 resolution={resolution} | project={project} | date_string={date_string}\")\n",
    "print(f\"📦 model={model} | base_model={base_model} | task={task}\")\n",
    "\n",
    "# ---- 🔒 Safety Check ----\n",
    "import os\n",
    "assert model == base_model or os.path.exists(model + \".pt\"), f\"Model path not found: {model}.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaebae7-54c5-4ded-b9ee-f91a99863bb7",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4846b3cc-5d79-4677-8a02-963193eb261e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ang/Seafile/TRex-tutorials-data/code\n",
      "New https://pypi.org/project/ultralytics/8.3.152 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.38 🚀 Python-3.12.7 torch-2.5.1.post4 CPU (Apple M2 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolo11n-seg.pt, data=/Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset/data.yaml, epochs=100, time=None, patience=0, batch=4, imgsz=1980, save=True, save_period=-1, cache=disk, device=cpu, workers=16, project=1980-yolo11n-seg-mosaic, name=2025-06-10-00_hexbugs-annotation-dataset_rebalanced_rebalanced-1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=1, format=tflite, keras=False, optimize=True, int8=False, dynamic=True, simplify=True, opset=None, workspace=None, nms=True, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0, translate=0.25, scale=0.25, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, bgr=0.0, mosaic=1, mixup=0.001, copy_paste=0.15, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=1980-yolo11n-seg-mosaic/2025-06-10-00_hexbugs-annotation-dataset_rebalanced_rebalanced-1\n",
      "Unable to revert mtime: /Library/Fonts\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    683635  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLO11n-seg summary: 355 layers, 2,842,803 parameters, 2,842,787 gradients, 10.4 GFLOPs\n",
      "\n",
      "Transferred 510/561 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir 1980-yolo11n-seg-mosaic/2025-06-10-00_hexbugs-annotation-dataset_rebalanced_rebalanced-1', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "WARNING ⚠️ imgsz=[1980] must be multiple of max stride 32, updating to [1984]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset/t\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset/train/labels.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB Disk): 100%|██████████| 28/28 [00:00<00:00, 245.16i\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset/val\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/ang/Seafile/TRex-tutorials-data/code/rebalanced_dataset/valid/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB Disk): 100%|██████████| 8/8 [00:00<00:00, 167.60it/s]\u001b[0m\n",
      "Plotting labels to 1980-yolo11n-seg-mosaic/2025-06-10-00_hexbugs-annotation-dataset_rebalanced_rebalanced-1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 1984 train, 1984 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m1980-yolo11n-seg-mosaic/2025-06-10-00_hexbugs-annotation-dataset_rebalanced_rebalanced-1\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100         0G     0.6119      1.443      4.319     0.8673         30   ^C\n"
     ]
    }
   ],
   "source": [
    "# Change to your working directory\n",
    "%cd {HOME}\n",
    "\n",
    "yolo_cmd = f\"\"\"\n",
    "yolo task={task} \\\n",
    "     mode=train \\\n",
    "     resume=False \\\n",
    "     model={model}.pt \\\n",
    "     data={dataset.location}/data.yaml \\\n",
    "     device=cpu \\\n",
    "     name={date_string} \\\n",
    "     project={project} \\\n",
    "     epochs={epochs} \\\n",
    "     imgsz={resolution} \\\n",
    "     batch={batch_size} \\\n",
    "     patience=0 \\\n",
    "     visualize=True \\\n",
    "     {parms}\n",
    "\"\"\"\n",
    "\n",
    "# ▶️ Run the command\n",
    "!{yolo_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844e85d-7b57-46e7-9d98-76e6edd32282",
   "metadata": {},
   "source": [
    "# 5) Locate trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a6411-66ab-455e-b8f5-42bb1b9e2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the working directory\n",
    "%cd {HOME}\n",
    "\n",
    "# List all subdirectories in the project folder\n",
    "all_subdirs = [project + '/' + d for d in os.listdir(project)]\n",
    "\n",
    "# Keep only those that contain a trained model\n",
    "all_subdirs = [d for d in all_subdirs if os.path.exists(d + \"/weights/last.pt\")]\n",
    "\n",
    "# Get the most recently modified subdirectory\n",
    "latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
    "\n",
    "# Construct the full path to the latest run\n",
    "full_path = HOME + \"/\" + latest_subdir\n",
    "\n",
    "print(project)\n",
    "print(latest_subdir)\n",
    "print(full_path)\n",
    "\n",
    "# Save training parameters to a parms.txt\n",
    "!echo \"{parms}\" > {latest_subdir}/parms.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c1bd8-11da-4ecb-bc79-a0429043c86a",
   "metadata": {},
   "source": [
    "### Select and Save Best YOLO Model Based on mAP Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a881fa-94fe-4fd4-9741-4bf2d174974e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latest_subdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if the best model weights file exists\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[43mlatest_subdir\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load training results CSV\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latest_subdir' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if the best model weights file exists\n",
    "print(os.path.exists(latest_subdir + \"/weights/best.pt\"))\n",
    "\n",
    "# Load training results CSV\n",
    "import pandas as pd\n",
    "csv = pd.read_csv(latest_subdir + \"/results.csv\")\n",
    "\n",
    "# Strip whitespace from column names\n",
    "for c in csv.columns:\n",
    "    csv = csv.rename(columns={c: c.strip()})\n",
    "    # print(c.strip())  # Optional: print cleaned column names\n",
    "\n",
    "# Check if metrics for both M (mask) and B (box) exist, and compute a weighted average\n",
    "if \"metrics/mAP50-95(M)\" in csv.columns:\n",
    "    # Combined score: weighted average of mask and box metrics (90% mAP50-95 + 10% mAP50)\n",
    "    combined = (csv[\"metrics/mAP50-95(M)\"] * 0.9 + csv[\"metrics/mAP50(M)\"] * 0.1) + \\\n",
    "               (csv[\"metrics/mAP50-95(B)\"] * 0.9 + csv[\"metrics/mAP50(B)\"] * 0.1)\n",
    "    \n",
    "    # Get index of best epoch based on combined score\n",
    "    index = combined.argmax()\n",
    "    \n",
    "    # Extract best mAP values for masks\n",
    "    best_map50_95 = csv[\"metrics/mAP50-95(M)\"].values[index]\n",
    "    best_map50 = csv[\"metrics/mAP50(M)\"].values[index]\n",
    "else:\n",
    "    # Only box metrics available; compute weighted score accordingly\n",
    "    combined = (csv[\"metrics/mAP50-95(B)\"] * 0.9 + csv[\"metrics/mAP50(B)\"] * 0.1)\n",
    "    index = combined.argmax()\n",
    "    \n",
    "    # Extract best mAP values for boxes\n",
    "    best_map50_95 = csv[\"metrics/mAP50-95(B)\"].values[index]\n",
    "    best_map50 = csv[\"metrics/mAP50(B)\"].values[index]\n",
    "\n",
    "# Define source path of best model\n",
    "from_path = latest_subdir + \"/weights/best.pt\"\n",
    "\n",
    "# Define destination path with project name, date, and mAP scores in filename\n",
    "to_path = HOME + \"/\" + project + \"-\" + date_string + \"-mAP5095_\" + str(best_map50_95) + \"-mAP50_\" + str(best_map50) + \".pt\"\n",
    "to_path = \"/content/\" + project + \"-\" + date_string + \"-mAP5095_\" + str(best_map50_95) + \"-mAP50_\" + str(best_map50) + \".pt\"\n",
    "\n",
    "# Log the copy action with source and destination paths\n",
    "print(\"copying from \", from_path, \"to\", to_path)\n",
    "\n",
    "# Copy the best model weights to the destination path with informative filename\n",
    "!cp {from_path} {to_path}\n",
    "\n",
    "# Upload the copied model file to a remote location using rsync with progress display\n",
    "!rsync --progress {to_path} {REMOTE_URL}/\n",
    "\n",
    "# Create a ZIP archive of the full training results folder\n",
    "!zip -r \"{HOME}/{latest_subdir}.zip\" \"{full_path}\"\n",
    "\n",
    "# Upload the zipped training results to the remote server using rsync with progress shown\n",
    "!rsync --progress \"{HOME}/{latest_subdir}.zip\" \"{REMOTE_URL}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affa1e7-8d34-4a0a-8dab-8ef3e163fdb2",
   "metadata": {},
   "source": [
    "### Training results plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cadea1d7-ce77-4343-94c1-e0cd84f8ca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ang/Seafile/TRex-tutorials-data/code\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'latest_subdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{HOME}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display the training results plot (e.g. loss and metrics curves)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m Image(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlatest_subdir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/results.png\u001b[39m\u001b[38;5;124m'\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latest_subdir' is not defined"
     ]
    }
   ],
   "source": [
    "# Change working directory to HOME\n",
    "%cd {HOME}\n",
    "\n",
    "# Display the training results plot (e.g. loss and metrics curves)\n",
    "Image(filename=f'{latest_subdir}/results.png', width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7102b-37c7-4dfa-98a3-1e06633482b4",
   "metadata": {},
   "source": [
    "### Sample batch of validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db475c-71e5-434e-b66b-cf0fc4e13bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to HOME\n",
    "%cd {HOME}\n",
    "\n",
    "# Display a sample batch of validation predictions (visual output of model)\n",
    "Image(filename=f'{latest_subdir}/val_batch0_pred.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e350f7-0cd5-4571-a32f-a9d67b779c00",
   "metadata": {},
   "source": [
    "# 6) Validate Custom Model\n",
    "\n",
    "This step runs **model validation** using the best trained checkpoint (`best.pt`) on the validation dataset defined in `data.yaml`. It evaluates the model's performance using standard YOLO metrics, such as:\n",
    "\n",
    "- **mAP50**: mean Average Precision at IoU threshold 0.5\n",
    "- **mAP50-95**: mean AP across IoU thresholds from 0.5 to 0.95\n",
    "- **Precision & Recall** for each class\n",
    "\n",
    "The validation results will be saved inside the specified project folder and include:\n",
    "\n",
    "- A `results.png` file with training/validation curves\n",
    "- A `confusion_matrix.png` for classification performance\n",
    "- A `val_batch0_pred.jpg` showing predicted bounding boxes on a sample batch\n",
    "\n",
    "You can use these visual and quantitative outputs to assess if the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d6420-89bc-4f21-9fed-5567bb1fc7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to HOME\n",
    "%cd {HOME}\n",
    "\n",
    "# Run YOLO validation on the best model checkpoint using the specified dataset and image size\n",
    "!yolo task={task} mode=val model={latest_subdir}/weights/best.pt data={dataset.location}/data.yaml project={project} imgsz={resolution} line_width=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd62054-f1b6-4b2f-8c59-b66f76f58d02",
   "metadata": {},
   "source": [
    "# 7) Run Inference on Validation Images\n",
    "\n",
    "This step performs **inference (prediction)** using the best trained YOLO model (`best.pt`) on the validation image set. It is useful to **visually inspect how the model performs** on real images after training.\n",
    "\n",
    "What this does:\n",
    "\n",
    "- Removes any existing `predict` folder to avoid clutter or overwriting previous predictions\n",
    "- Runs YOLO in `predict` mode using:\n",
    "  - The best model checkpoint\n",
    "  - Images from the validation set\n",
    "  - A low confidence threshold (`conf=0.1`) to allow more predictions for visual inspection\n",
    "  - The specified image size (`imgsz`)\n",
    "- Saves predicted images (with boxes, masks, or keypoints depending on the task) in a new folder under the project directory: `runs/predict`\n",
    "\n",
    "This is especially helpful for qualitatively checking the model's detection performance, spotting failure cases, or selecting images for visualization or presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a565059-e26d-44be-aba6-3ac94941e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory to HOME\n",
    "%cd {HOME}\n",
    "\n",
    "# Remove any previous YOLO prediction results to avoid overwriting conflicts\n",
    "%rm -rf {latest_subdir}/../predict\n",
    "\n",
    "# Run YOLO prediction on validation images using the best model checkpoint\n",
    "!yolo task={task} mode=predict model={latest_subdir}/weights/best.pt project={project} name=predict conf=0.1 source={dataset.location}/valid/images save=true imgsz={resolution} line_width=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412a87e-9108-4329-9189-50c5608a00cd",
   "metadata": {},
   "source": [
    "### Zip and Save Prediction Results\n",
    "\n",
    "This step creates a ZIP archive of the prediction results generated in the previous step. The archive is saved in your home directory and named using the training subdirectory name (to make it easy to track which model it came from).\n",
    "\n",
    "This makes it simple to download, share, or upload the predictions for external use (e.g., for presentations, manual inspection, or further analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef6117-8346-4c6f-9720-2df98a425a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ZIP archive of the YOLO prediction results\n",
    "# The ZIP file will be named using the current training subdirectory name to keep it traceable\n",
    "zipname = latest_subdir.replace('/', '_')\n",
    "!zip -r \"{HOME}/prediction_{zipname}.zip\" {HOME}/{project}/predict -i \"{HOME}/{project}/predict/*\"\n",
    "\n",
    "# Upload the zipped prediction results to the remote server using rsync with progress feedback\n",
    "!rsync --progress \"{HOME}/prediction_{zipname}.zip\" \"{REMOTE_URL}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7347c-f335-4256-8d5c-0bb69307a994",
   "metadata": {},
   "source": [
    "# 10) Display Sample Predictions\n",
    "\n",
    "This step randomly selects and displays 5 predicted images from the `predict` folder.\n",
    "\n",
    "Each image includes the model's output (e.g., bounding boxes, masks, or keypoints) overlaid on the validation images.  \n",
    "It provides a quick **visual inspection** of model performance across different examples.  \n",
    "\n",
    "This qualitative check helps identify:\n",
    "- How well the model localizes objects\n",
    "- Possible false positives or negatives\n",
    "- Class confusion or missed detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c27c8-19ca-4da2-9e5e-cdbae4fd3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 5 predicted images from the YOLO prediction output folder\n",
    "files = np.random.choice(glob.glob(f'{HOME}/{project}/predict/*.jpg'), size=5)\n",
    "print(files.shape)\n",
    "\n",
    "# Display each selected image and print a newline for spacing\n",
    "for image_path in files:\n",
    "    display(Image(filename=image_path, height=600))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a6df7-20d2-4d55-81c0-8778398a76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Wait for 30 seconds (e.g., to ensure all background tasks finish before disconnecting)\n",
    "time.sleep(30)\n",
    "\n",
    "# Gracefully disconnect the current Colab runtime session\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c27a6e-b52c-45fd-90a2-57324a8152bc",
   "metadata": {},
   "source": [
    "## 🏆 Congratulations\n",
    "\n",
    "### Find more learning resources here\n",
    "\n",
    "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
    "\n",
    "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
    "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
    "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
    "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
    "\n",
    "### Convert data formats\n",
    "\n",
    "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
    "\n",
    "### Connect computer vision to your project logic\n",
    "\n",
    "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51f5b4-4947-46b7-a453-0e613b512668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
